{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e17e1aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Author:\n",
    "    zanshuxun, zanshuxun@aliyun.com\n",
    "    songwei, magic_24k@163.com\n",
    "\n",
    "Reference:\n",
    "    [1] [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "from deepctr_torch.models.basemodel import BaseModel\n",
    "from deepctr_torch.inputs import combined_dnn_input, embedding_lookup, maxlen_lookup\n",
    "from deepctr_torch.layers import DNN, PredictionLayer, CIN, concat_fun, InteractingLayer\n",
    "from deepctr_torch.layers.sequence import AttentionSequencePoolingLayer\n",
    "import pandas as pd\n",
    "\n",
    "class MMOELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multi-gate Mixture-of-Experts layer in MMOE model\n",
    "      Input shape\n",
    "        - 2D tensor with shape: ``(batch_size,units)``.\n",
    "\n",
    "      Output shape\n",
    "        - A list with **num_tasks** elements, which is a 2D tensor with shape: ``(batch_size, output_dim)`` .\n",
    "\n",
    "      Arguments\n",
    "        - **input_dim** : Positive integer, dimensionality of input features.\n",
    "        - **num_tasks**: integer, the number of tasks, equal to the number of outputs.\n",
    "        - **num_experts**: integer, the number of experts.\n",
    "        - **output_dim**: integer, the dimension of each output of MMOELayer.\n",
    "\n",
    "    References\n",
    "      - [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_tasks, num_experts, output_dim):\n",
    "        super(MMOELayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        self.output_dim = output_dim\n",
    "        self.expert_network = nn.Linear(self.input_dim, self.num_experts * self.output_dim, bias=True)\n",
    "        self.gating_networks = nn.ModuleList(\n",
    "            [nn.Linear(self.input_dim, self.num_experts, bias=False) for _ in range(self.num_tasks)])\n",
    "        # initial model\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        expert_out = self.expert_network(inputs)\n",
    "        expert_out = expert_out.reshape([-1, self.output_dim, self.num_experts])\n",
    "        for i in range(self.num_tasks):\n",
    "            gate_out = self.gating_networks[i](inputs)\n",
    "            gate_out = gate_out.softmax(1).unsqueeze(-1)\n",
    "            output = torch.bmm(expert_out, gate_out).squeeze()\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MMOE(BaseModel):\n",
    "    \"\"\"Instantiates the Multi-gate Mixture-of-Experts architecture.\n",
    "\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n",
    "    :param tasks: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n",
    "    :param num_experts: integer, number of experts.\n",
    "    :param expert_dim: integer, the hidden units of each expert.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param task_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "\n",
    "    :return: A PyTorch model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dnn_feature_columns, history_feature_list, num_tasks, tasks, num_experts=4, expert_dim=8, dnn_hidden_units=(128, 128),\n",
    "                 l2_reg_embedding=1e-5, l2_reg_dnn=1e-5, init_std=0.0001, task_dnn_units=None, seed=1024, dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, device='cpu', gpus=[0, 1]):\n",
    "        \n",
    "        super(MMOE, self).__init__(linear_feature_columns=[], dnn_feature_columns=dnn_feature_columns,\n",
    "                                   l2_reg_embedding=l2_reg_embedding, seed=seed, device=device)\n",
    "        if num_tasks <= 1:\n",
    "            raise ValueError(\"num_tasks must be greater than 1\")\n",
    "        if len(tasks) != num_tasks:\n",
    "            raise ValueError(\"num_tasks must be equal to the length of tasks\")\n",
    "        for task in tasks:\n",
    "            if task not in ['binary', 'regression']:\n",
    "                raise ValueError(\"task must be binary or regression, {} is illegal\".format(task))\n",
    "        self.sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        self.varlen_sparse_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        self.dense_feature_columns = list(\n",
    "            filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        \n",
    "        # atten tag key\n",
    "        self.history_feature_list = history_feature_list\n",
    "        self.item_features = history_feature_list\n",
    "\n",
    "        self.history_feature_columns = []\n",
    "        self.sparse_varlen_feature_columns = []\n",
    "        self.history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n",
    "        for fc in self.varlen_sparse_feature_columns:\n",
    "            feature_name = fc.name\n",
    "            if feature_name in self.history_fc_names:\n",
    "                self.history_feature_columns.append(fc)\n",
    "            else:\n",
    "                self.sparse_varlen_feature_columns.append(fc)\n",
    "        \n",
    "        # din component\n",
    "#         att_emb_dim = self._compute_interest_dim()\n",
    "#         use_negsampling = False\n",
    "#         gru_type=\"GRU\"\n",
    "#         att_hidden_size=(128, 64)\n",
    "#         self.use_negsampling = use_negsampling\n",
    "#         self.gru_type = gru_type\n",
    "#         att_activation='relu'\n",
    "#         att_weight_normalization=False\n",
    "#         self.attention = AttentionSequencePoolingLayer(att_hidden_units=att_hidden_size,\n",
    "#                                                        embedding_dim=att_emb_dim,\n",
    "#                                                        att_activation=att_activation,\n",
    "#                                                        return_score=False,\n",
    "#                                                        supports_masking=False,\n",
    "#                                                        weight_normalization=att_weight_normalization)\n",
    "        # DIEN\n",
    "#         self.alpha=1.0\n",
    "#         self.interest_extractor = InterestExtractor(input_size=att_emb_dim, use_neg=use_negsampling, init_std=init_std)\n",
    "#         self.interest_evolution = InterestEvolving(\n",
    "#             input_size=att_emb_dim,\n",
    "#             gru_type=gru_type,\n",
    "#             use_neg=use_negsampling,\n",
    "#             init_std=init_std,\n",
    "#             att_hidden_size=att_hidden_size,\n",
    "#             att_activation=att_activation,\n",
    "#             att_weight_normalization=att_weight_normalization)\n",
    "#         self.din_linear = nn.Linear(80, 1, bias=False).to(device)\n",
    "        \n",
    "        # 加入cin\n",
    "        cin_layer_size=(256, 128, 64)\n",
    "        self.cin_layer_size=cin_layer_size\n",
    "        cin_split_half=True\n",
    "        cin_activation='relu'\n",
    "        l2_reg_cin=1e-5\n",
    "        self.use_cin = len(self.cin_layer_size) > 0 and len(dnn_feature_columns) > 0\n",
    "        if self.use_cin:\n",
    "#             field_num = len(self.embedding_dict)\n",
    "            field_num = len(self.sparse_feature_columns)\n",
    "\n",
    "            if cin_split_half == True:\n",
    "                self.featuremap_num = sum(\n",
    "                    cin_layer_size[:-1]) // 2 + cin_layer_size[-1]\n",
    "            else:\n",
    "                self.featuremap_num = sum(cin_layer_size)\n",
    "            self.cin = CIN(field_num, cin_layer_size,\n",
    "                           cin_activation, cin_split_half, l2_reg_cin, seed, device=device)\n",
    "            self.cin_linear = nn.Linear(self.featuremap_num, 1, bias=False).to(device)\n",
    "            self.add_regularization_weight(filter(lambda x: 'weight' in x[0], self.cin.named_parameters()),\n",
    "                                           l2=l2_reg_cin)\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "        # multi-head atten\n",
    "        att_embedding_size=30\n",
    "        att_head_num=10\n",
    "        att_layer_num=3\n",
    "        att_res=True\n",
    "        self.int_layers = nn.ModuleList(\n",
    "            [InteractingLayer(self.embedding_size if i == 0 else att_embedding_size * att_head_num,\n",
    "                              att_embedding_size, att_head_num, att_res, device=device) for i in range(att_layer_num)])\n",
    "        \n",
    "        if len(dnn_hidden_units) and att_layer_num > 0:\n",
    "            dnn_linear_in_feature = dnn_hidden_units[-1] + \\\n",
    "                                    field_num * att_embedding_size * att_head_num\n",
    "        elif len(dnn_hidden_units) > 0:\n",
    "            dnn_linear_in_feature = dnn_hidden_units[-1]\n",
    "        elif att_layer_num > 0:\n",
    "            dnn_linear_in_feature = field_num * att_embedding_size * att_head_num\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        print('sparse_features: ', self.sparse_feature_columns)\n",
    "        print('varlen_features: ', self.varlen_sparse_feature_columns)\n",
    "        \n",
    "         # MMOE\n",
    "        self.tasks = tasks\n",
    "        self.task_dnn_units = task_dnn_units\n",
    "        self.dnn = DNN(\n",
    "#             self.compute_input_dim(self.sparse_feature_columns + self.dense_feature_columns) +16 + dnn_linear_in_feature + self.featuremap_num, \n",
    "            self.compute_input_dim(self.sparse_feature_columns + self.dense_feature_columns),   \n",
    "            dnn_hidden_units,\n",
    "            activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                       init_std=init_std, device=device)\n",
    "        self.mmoe_layer = MMOELayer(dnn_hidden_units[-1]+1600+800, num_tasks, num_experts, expert_dim)\n",
    "        if task_dnn_units is not None:\n",
    "            # the last layer of task_dnn should be expert_dim\n",
    "            self.task_dnn = nn.ModuleList([DNN(expert_dim, task_dnn_units+(expert_dim,)) for _ in range(num_tasks)])\n",
    "        self.tower_network = nn.ModuleList([nn.Linear(expert_dim, 1, bias=False) for _ in range(num_tasks)])\n",
    "        self.out = nn.ModuleList([PredictionLayer(task) for task in self.tasks])\n",
    "        self.to(device)\n",
    "    \n",
    "    def _compute_interest_dim(self):\n",
    "        interest_dim = 0\n",
    "        for feat in self.sparse_feature_columns:\n",
    "            if feat.name in self.history_feature_list:\n",
    "                interest_dim += feat.embedding_dim\n",
    "        return interest_dim\n",
    "    def _get_emb(self, X):\n",
    "        # history feature columns : pos, neg\n",
    "        \n",
    "        history_feature_columns = []\n",
    "        neg_history_feature_columns = []\n",
    "        sparse_varlen_feature_columns = []\n",
    "        history_fc_names = list(map(lambda x: \"hist_\" + x, self.item_features))\n",
    "        neg_history_fc_names = list(map(lambda x: \"neg_\" + x, history_fc_names))\n",
    "        for fc in self.varlen_sparse_feature_columns:\n",
    "            feature_name = fc.name\n",
    "            if feature_name in history_fc_names:\n",
    "                history_feature_columns.append(fc)\n",
    "            elif feature_name in neg_history_fc_names:\n",
    "                neg_history_feature_columns.append(fc)\n",
    "            else:\n",
    "                sparse_varlen_feature_columns.append(fc)\n",
    "\n",
    "        # convert input to emb\n",
    "        features = self.feature_index\n",
    "        query_emb_list = embedding_lookup(X, self.embedding_dict, features, self.sparse_feature_columns,\n",
    "                                          return_feat_list=self.item_features, to_list=True)\n",
    "        # [batch_size, dim]\n",
    "        query_emb = torch.squeeze(concat_fun(query_emb_list), 1)\n",
    "\n",
    "        keys_emb_list = embedding_lookup(X, self.embedding_dict, features, history_feature_columns,\n",
    "                                         return_feat_list=history_fc_names, to_list=True)\n",
    "        # [batch_size, max_len, dim]\n",
    "        keys_emb = concat_fun(keys_emb_list)\n",
    "\n",
    "        keys_length_feature_name = [feat.length_name for feat in self.varlen_sparse_feature_columns if\n",
    "                                    feat.length_name is not None]\n",
    "        # [batch_size]\n",
    "        keys_length = torch.squeeze(maxlen_lookup(X, features, keys_length_feature_name), 1)\n",
    "\n",
    "        if self.use_negsampling:\n",
    "            neg_keys_emb_list = embedding_lookup(X, self.embedding_dict, features, neg_history_feature_columns,\n",
    "                                                 return_feat_list=neg_history_fc_names, to_list=True)\n",
    "            neg_keys_emb = concat_fun(neg_keys_emb_list)\n",
    "        else:\n",
    "            neg_keys_emb = None\n",
    "\n",
    "        return query_emb, keys_emb, neg_keys_emb, keys_length\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "#         print(self.embedding_dict)\n",
    "        _, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                           self.embedding_dict)\n",
    "        sparse_embedding_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.sparse_feature_columns,\n",
    "                                              to_list=True)\n",
    "        #DIN\n",
    "\n",
    "#         query_emb_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.sparse_feature_columns,\n",
    "#                                           return_feat_list=self.history_feature_list, to_list=True)\n",
    "#         keys_emb_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.history_feature_columns,\n",
    "#                                          return_feat_list=self.history_fc_names, to_list=True)\n",
    "#         query_emb = torch.cat(query_emb_list, dim=-1)                     # [B, 1, E]\n",
    "#         keys_emb = torch.cat(keys_emb_list, dim=-1)                       # [B, T, E]\n",
    "#         keys_length_feature_name = [feat.length_name for feat in self.varlen_sparse_feature_columns if\n",
    "#                                     feat.length_name is not None]\n",
    "#         keys_length = torch.squeeze(maxlen_lookup(X, self.feature_index, keys_length_feature_name), 1)  # [B, 1]\n",
    "#         hist = self.attention(query_emb, keys_emb, keys_length)           # [B, 1, E]\n",
    "        #DIEN\n",
    "#         query_emb, keys_emb, neg_keys_emb, keys_length = self._get_emb(X)\n",
    "#         masked_interest, aux_loss = self.interest_extractor(keys_emb, keys_length, neg_keys_emb)\n",
    "#         self.add_auxiliary_loss(aux_loss, self.alpha)\n",
    "#         hist = self.interest_evolution(query_emb, masked_interest, keys_length)\n",
    "\n",
    "#         din_logit = self.din_linear(hist).squeeze(1)\n",
    "#         din_logit = self.din_out(din_out)\n",
    "        \n",
    "#         加入cin模块\n",
    "        if self.use_cin:\n",
    "            cin_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            cin_output = self.cin(cin_input) # 1024, 256\n",
    "            cin_logit = self.cin_linear(cin_output)\n",
    "            \n",
    "#         print('cin_logit: ', cin_logit)  \n",
    "#         print('din_logit: ', din_logit, din_logit.shape)\n",
    "#         muti-head\n",
    "        att_input = concat_fun(sparse_embedding_list, axis=1)\n",
    "        for layer in self.int_layers:\n",
    "            att_input = layer(att_input)\n",
    "        att_output = torch.flatten(att_input, start_dim=1)\n",
    "\n",
    "        # dnn\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list) # 1024, 101\n",
    "#         print('dnn_input: ', dnn_input.shape)\n",
    "#         print('hist: ', hist.shape)\n",
    "#         dnn_input = torch.cat((dnn_input, hist.squeeze(1)), dim=-1)\n",
    "#         dnn_input = torch.cat((dnn_input, att_output), dim=-1)\n",
    "#         dnn_input = torch.cat((dnn_input, cin_output), dim=-1)\n",
    "        \n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        dnn_output = concat_fun([att_output, dnn_output])\n",
    "        mmoe_outs = self.mmoe_layer(dnn_output)\n",
    "        if self.task_dnn_units is not None:\n",
    "            mmoe_outs = [self.task_dnn[i](mmoe_out) for i, mmoe_out in enumerate(mmoe_outs)]\n",
    "\n",
    "        task_outputs = []\n",
    "        for i, mmoe_out in enumerate(mmoe_outs):\n",
    "            logit = self.tower_network[i](mmoe_out) + cin_logit\n",
    "#             logit = self.tower_network[i](mmoe_out)\n",
    "            output = self.out[i](logit)\n",
    "            task_outputs.append(output)\n",
    "#         print(cin_logit.shape, din_logit.shape)\n",
    "#         print(task_outputs.shape)\n",
    "        task_outputs = torch.cat(task_outputs, -1)\n",
    "        return task_outputs\n",
    "\n",
    "\n",
    "class InterestExtractor(nn.Module):\n",
    "    def __init__(self, input_size, use_neg=False, init_std=0.001, device='cpu'):\n",
    "        super(InterestExtractor, self).__init__()\n",
    "        self.use_neg = use_neg\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=input_size, batch_first=True)\n",
    "        if self.use_neg:\n",
    "            self.auxiliary_net = DNN(input_size * 2, [100, 50, 1], 'sigmoid', init_std=init_std, device=device)\n",
    "        for name, tensor in self.gru.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, keys, keys_length, neg_keys=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        keys: 3D tensor, [B, T, H]\n",
    "        keys_length: 1D tensor, [B]\n",
    "        neg_keys: 3D tensor, [B, T, H]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        masked_interests: 2D tensor, [b, H]\n",
    "        aux_loss: [1]\n",
    "        \"\"\"\n",
    "        batch_size, max_length, dim = keys.size()\n",
    "        zero_outputs = torch.zeros(batch_size, dim, device=keys.device)\n",
    "        aux_loss = torch.zeros((1,), device=keys.device)\n",
    "\n",
    "        # create zero mask for keys_length, to make sure 'pack_padded_sequence' safe\n",
    "        mask = keys_length > 0\n",
    "        masked_keys_length = keys_length[mask]\n",
    "\n",
    "        # batch_size validation check\n",
    "        if masked_keys_length.shape[0] == 0:\n",
    "            return zero_outputs,\n",
    "\n",
    "        masked_keys = torch.masked_select(keys, mask.view(-1, 1, 1)).view(-1, max_length, dim)\n",
    "\n",
    "        packed_keys = pack_padded_sequence(masked_keys, lengths=masked_keys_length, batch_first=True,\n",
    "                                           enforce_sorted=False)\n",
    "        packed_interests, _ = self.gru(packed_keys)\n",
    "#         print('keys_len: ', keys_length.max(), keys_length)\n",
    "#         print(max_length, masked_keys_length, masked_keys.shape)\n",
    "#         print(packed_interests, packed_interests.batch_sizes, packed_interests.batch_sizes.size(0))\n",
    "        interests, _ = pad_packed_sequence(packed_interests, batch_first=True, padding_value=0.0,\n",
    "                                           total_length=max_length)\n",
    "\n",
    "        if self.use_neg and neg_keys is not None:\n",
    "            masked_neg_keys = torch.masked_select(neg_keys, mask.view(-1, 1, 1)).view(-1, max_length, dim)\n",
    "            aux_loss = self._cal_auxiliary_loss(\n",
    "                interests[:, :-1, :],\n",
    "                masked_keys[:, 1:, :],\n",
    "                masked_neg_keys[:, 1:, :],\n",
    "                masked_keys_length - 1)\n",
    "\n",
    "        return interests, aux_loss\n",
    "\n",
    "    def _cal_auxiliary_loss(self, states, click_seq, noclick_seq, keys_length):\n",
    "        # keys_length >= 1\n",
    "        mask_shape = keys_length > 0\n",
    "        keys_length = keys_length[mask_shape]\n",
    "        if keys_length.shape[0] == 0:\n",
    "            return torch.zeros((1,), device=states.device)\n",
    "\n",
    "        _, max_seq_length, embedding_size = states.size()\n",
    "        states = torch.masked_select(states, mask_shape.view(-1, 1, 1)).view(-1, max_seq_length, embedding_size)\n",
    "        click_seq = torch.masked_select(click_seq, mask_shape.view(-1, 1, 1)).view(-1, max_seq_length, embedding_size)\n",
    "        noclick_seq = torch.masked_select(noclick_seq, mask_shape.view(-1, 1, 1)).view(-1, max_seq_length,\n",
    "                                                                                       embedding_size)\n",
    "        batch_size = states.size()[0]\n",
    "\n",
    "        mask = (torch.arange(max_seq_length, device=states.device).repeat(\n",
    "            batch_size, 1) < keys_length.view(-1, 1)).float()\n",
    "\n",
    "        click_input = torch.cat([states, click_seq], dim=-1)\n",
    "        noclick_input = torch.cat([states, noclick_seq], dim=-1)\n",
    "        embedding_size = embedding_size * 2\n",
    "\n",
    "        click_p = self.auxiliary_net(click_input.view(\n",
    "            batch_size * max_seq_length, embedding_size)).view(\n",
    "            batch_size, max_seq_length)[mask > 0].view(-1, 1)\n",
    "        click_target = torch.ones(\n",
    "            click_p.size(), dtype=torch.float, device=click_p.device)\n",
    "\n",
    "        noclick_p = self.auxiliary_net(noclick_input.view(\n",
    "            batch_size * max_seq_length, embedding_size)).view(\n",
    "            batch_size, max_seq_length)[mask > 0].view(-1, 1)\n",
    "        noclick_target = torch.zeros(\n",
    "            noclick_p.size(), dtype=torch.float, device=noclick_p.device)\n",
    "\n",
    "        loss = F.binary_cross_entropy(\n",
    "            torch.cat([click_p, noclick_p], dim=0),\n",
    "            torch.cat([click_target, noclick_target], dim=0))\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class InterestEvolving(nn.Module):\n",
    "    __SUPPORTED_GRU_TYPE__ = ['GRU', 'AIGRU', 'AGRU', 'AUGRU']\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 gru_type='GRU',\n",
    "                 use_neg=False,\n",
    "                 init_std=0.001,\n",
    "                 att_hidden_size=(64, 16),\n",
    "                 att_activation='sigmoid',\n",
    "                 att_weight_normalization=False):\n",
    "        super(InterestEvolving, self).__init__()\n",
    "        if gru_type not in InterestEvolving.__SUPPORTED_GRU_TYPE__:\n",
    "            raise NotImplementedError(\"gru_type: {gru_type} is not supported\")\n",
    "        self.gru_type = gru_type\n",
    "        self.use_neg = use_neg\n",
    "\n",
    "        if gru_type == 'GRU':\n",
    "            self.attention = AttentionSequencePoolingLayer(embedding_dim=input_size,\n",
    "                                                           att_hidden_units=att_hidden_size,\n",
    "                                                           att_activation=att_activation,\n",
    "                                                           weight_normalization=att_weight_normalization,\n",
    "                                                           return_score=False)\n",
    "            self.interest_evolution = nn.GRU(input_size=input_size, hidden_size=input_size, batch_first=True)\n",
    "        elif gru_type == 'AIGRU':\n",
    "            self.attention = AttentionSequencePoolingLayer(embedding_dim=input_size,\n",
    "                                                           att_hidden_units=att_hidden_size,\n",
    "                                                           att_activation=att_activation,\n",
    "                                                           weight_normalization=att_weight_normalization,\n",
    "                                                           return_score=True)\n",
    "            self.interest_evolution = nn.GRU(input_size=input_size, hidden_size=input_size, batch_first=True)\n",
    "        elif gru_type == 'AGRU' or gru_type == 'AUGRU':\n",
    "            self.attention = AttentionSequencePoolingLayer(embedding_dim=input_size,\n",
    "                                                           att_hidden_units=att_hidden_size,\n",
    "                                                           att_activation=att_activation,\n",
    "                                                           weight_normalization=att_weight_normalization,\n",
    "                                                           return_score=True)\n",
    "            self.interest_evolution = DynamicGRU(input_size=input_size, hidden_size=input_size,\n",
    "                                                 gru_type=gru_type)\n",
    "        for name, tensor in self.interest_evolution.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_last_state(states, keys_length):\n",
    "        # states [B, T, H]\n",
    "        batch_size, max_seq_length, hidden_size = states.size()\n",
    "\n",
    "        mask = (torch.arange(max_seq_length, device=keys_length.device).repeat(\n",
    "            batch_size, 1) == (keys_length.view(-1, 1) - 1))\n",
    "\n",
    "        return states[mask]\n",
    "\n",
    "    def forward(self, query, keys, keys_length, mask=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: 2D tensor, [B, H]\n",
    "        keys: (masked_interests), 3D tensor, [b, T, H]\n",
    "        keys_length: 1D tensor, [B]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs: 2D tensor, [B, H]\n",
    "        \"\"\"\n",
    "        batch_size, dim = query.size()\n",
    "        max_length = keys.size()[1]\n",
    "\n",
    "        # check batch validation\n",
    "        zero_outputs = torch.zeros(batch_size, dim, device=query.device)\n",
    "        mask = keys_length > 0\n",
    "        # [B] -> [b]\n",
    "        keys_length = keys_length[mask]\n",
    "        if keys_length.shape[0] == 0:\n",
    "            return zero_outputs\n",
    "\n",
    "        # [B, H] -> [b, 1, H]\n",
    "        query = torch.masked_select(query, mask.view(-1, 1)).view(-1, dim).unsqueeze(1)\n",
    "\n",
    "        if self.gru_type == 'GRU':\n",
    "            packed_keys = pack_padded_sequence(keys, lengths=keys_length, batch_first=True, enforce_sorted=False)\n",
    "            packed_interests, _ = self.interest_evolution(packed_keys)\n",
    "            interests, _ = pad_packed_sequence(packed_interests, batch_first=True, padding_value=0.0,\n",
    "                                               total_length=max_length)\n",
    "            outputs = self.attention(query, interests, keys_length.unsqueeze(1))  # [b, 1, H]\n",
    "            outputs = outputs.squeeze(1)  # [b, H]\n",
    "        elif self.gru_type == 'AIGRU':\n",
    "            att_scores = self.attention(query, keys, keys_length.unsqueeze(1))  # [b, 1, T]\n",
    "            interests = keys * att_scores.transpose(1, 2)  # [b, T, H]\n",
    "            packed_interests = pack_padded_sequence(interests, lengths=keys_length, batch_first=True,\n",
    "                                                    enforce_sorted=False)\n",
    "            _, outputs = self.interest_evolution(packed_interests)\n",
    "            outputs = outputs.squeeze(0) # [b, H]\n",
    "        elif self.gru_type == 'AGRU' or self.gru_type == 'AUGRU':\n",
    "            att_scores = self.attention(query, keys, keys_length.unsqueeze(1)).squeeze(1)  # [b, T]\n",
    "            packed_interests = pack_padded_sequence(keys, lengths=keys_length, batch_first=True,\n",
    "                                                    enforce_sorted=False)\n",
    "            packed_scores = pack_padded_sequence(att_scores, lengths=keys_length, batch_first=True,\n",
    "                                                 enforce_sorted=False)\n",
    "            outputs = self.interest_evolution(packed_interests, packed_scores)\n",
    "            outputs, _ = pad_packed_sequence(outputs, batch_first=True, padding_value=0.0, total_length=max_length)\n",
    "            # pick last state\n",
    "            outputs = InterestEvolving._get_last_state(outputs, keys_length) # [b, H]\n",
    "        # [b, H] -> [B, H]\n",
    "        zero_outputs[mask] = outputs\n",
    "        return zero_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93becab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = [\n",
    " 'videoplayseconds',\n",
    "#  'userid_5day_count',\n",
    "#  'userid_5day_play_times_mean',\n",
    "#  'userid_5day_play_mean',\n",
    "#  'userid_5day_stay_mean',\n",
    "#  'userid_5day_read_comment_sum',\n",
    "#  'userid_5day_read_comment_mean',\n",
    "#  'userid_5day_read_comment_count',\n",
    "#  'userid_5day_like_sum',\n",
    "#  'userid_5day_like_mean',\n",
    "#  'userid_5day_like_count',\n",
    "#  'userid_5day_click_avatar_sum',\n",
    "#  'userid_5day_click_avatar_mean',\n",
    "#  'userid_5day_click_avatar_count',\n",
    "#  'userid_5day_forward_sum',\n",
    "#  'userid_5day_forward_mean',\n",
    "#  'userid_5day_forward_count',\n",
    "#  'userid_5day_favorite_sum',\n",
    "#  'userid_5day_favorite_mean',\n",
    "#  'userid_5day_favorite_count',\n",
    "#  'userid_5day_comment_sum',\n",
    "#  'userid_5day_comment_mean',\n",
    "#  'userid_5day_comment_count',\n",
    "#  'userid_5day_follow_sum',\n",
    "#  'userid_5day_follow_mean',\n",
    "#  'userid_5day_follow_count',\n",
    "#  'feedid_5day_count',\n",
    "#  'feedid_5day_play_times_mean',\n",
    "#  'feedid_5day_play_mean',\n",
    "#  'feedid_5day_stay_mean',\n",
    "#  'feedid_5day_read_comment_sum',\n",
    "#  'feedid_5day_read_comment_mean',\n",
    "#  'feedid_5day_read_comment_count',\n",
    "#  'feedid_5day_like_sum',\n",
    "#  'feedid_5day_like_mean',\n",
    "#  'feedid_5day_like_count',\n",
    "#  'feedid_5day_click_avatar_sum',\n",
    "#  'feedid_5day_click_avatar_mean',\n",
    "#  'feedid_5day_click_avatar_count',\n",
    "#  'feedid_5day_forward_sum',\n",
    "#  'feedid_5day_forward_mean',\n",
    "#  'feedid_5day_forward_count',\n",
    "#  'feedid_5day_favorite_sum',\n",
    "#  'feedid_5day_favorite_mean',\n",
    "#  'feedid_5day_favorite_count',\n",
    "#  'feedid_5day_comment_sum',\n",
    "#  'feedid_5day_comment_mean',\n",
    "#  'feedid_5day_comment_count',\n",
    "#  'feedid_5day_follow_sum',\n",
    "#  'feedid_5day_follow_mean',\n",
    "#  'feedid_5day_follow_count',\n",
    "#  'authorid_5day_count',\n",
    "#  'authorid_5day_play_times_mean',\n",
    "#  'authorid_5day_play_mean',\n",
    "#  'authorid_5day_stay_mean',\n",
    "#  'authorid_5day_read_comment_sum',\n",
    "#  'authorid_5day_read_comment_mean',\n",
    "#  'authorid_5day_read_comment_count',\n",
    "#  'authorid_5day_like_sum',\n",
    "#  'authorid_5day_like_mean',\n",
    "#  'authorid_5day_like_count',\n",
    "#  'authorid_5day_click_avatar_sum',\n",
    "#  'authorid_5day_click_avatar_mean',\n",
    "#  'authorid_5day_click_avatar_count',\n",
    "#  'authorid_5day_forward_sum',\n",
    "#  'authorid_5day_forward_mean',\n",
    "#  'authorid_5day_forward_count',\n",
    "#  'authorid_5day_favorite_sum',\n",
    "#  'authorid_5day_favorite_mean',\n",
    "#  'authorid_5day_favorite_count',\n",
    "#  'authorid_5day_comment_sum',\n",
    "#  'authorid_5day_comment_mean',\n",
    "#  'authorid_5day_comment_count',\n",
    "#  'authorid_5day_follow_sum',\n",
    "#  'authorid_5day_follow_mean',\n",
    "#  'authorid_5day_follow_count',\n",
    "#  'userid_count',\n",
    "#  'feedid_count',\n",
    "#  'authorid_count',\n",
    "#  'userid_in_feedid_nunique',\n",
    "#  'feedid_in_userid_nunique',\n",
    "#  'userid_in_authorid_nunique',\n",
    "#  'authorid_in_userid_nunique',\n",
    "#  'userid_authorid_count',\n",
    "#  'userid_in_authorid_count_prop',\n",
    "#  'authorid_in_userid_count_prop',\n",
    "#  'videoplayseconds_in_userid_mean',\n",
    "#  'feedid_in_authorid_nunique',\n",
    "#  'bgm_song_id',\n",
    "#  'bgm_singer_id'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b341d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6914b11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use features:  ['bgm_song_id', 'bgm_singer_id', 'userid', 'feedid', 'authorid', 'device', 'videoplayseconds', 'user_embedding_normal', 'user_embedding_adjust']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import sys\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "BASE_DIR = '.'\n",
    "sys.path.append(os.path.join(BASE_DIR, '../../config'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../model'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "from config import *\n",
    "from time import time\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names, VarLenSparseFeat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datatable as dt\n",
    "# from mmoe import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# 训练相关参数设置\n",
    "ONLINE_FLAG = True # 是否准备线上提交\n",
    "\n",
    "# 指定GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "\n",
    "vocab_dict = {\n",
    "    'bgm_song_id': 25158+1,\n",
    "    'bgm_singer_id': 17499+1,\n",
    "    'userid': 199999,\n",
    "    'feedid': 112871+1,\n",
    "    'authorid': 18788+1,\n",
    "    'device' : 3\n",
    "}\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "epochs = 1\n",
    "batch_size = 1024\n",
    "embedding_dim = 50\n",
    "max_hist_seq_len = 100\n",
    "\n",
    "target = ['read_comment', 'like', 'click_avatar', 'forward', 'comment', 'favorite', 'follow']\n",
    "tagids = ['manual_tag_' + str(tagid) for tagid in range(11)] # 11\n",
    "keyids = ['manual_key_' + str(keyid) for keyid in range(11)] # 18\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "# dense_features += ['videoplayseconds', ]\n",
    "\n",
    "feed = dt.fread(FEED_INFO)\n",
    "feed = feed.to_pandas()\n",
    "# tag = dt.fread(FEATURE_PATH + '/feed_info_tags_keys_des_seq_len.csv')\n",
    "# tag = tag.to_pandas()[tagids + keyids + ['feedid', 'tag_seq_len', 'key_seq_len']]\n",
    "\n",
    "pkl = open(FEATURE_PATH + '/user_encoder.pkl', 'rb')\n",
    "userid_map = pickle.load(pkl)\n",
    "pkl.close()\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "# feed_embs = pd.read_csv(FEATURE_PATH + '/feed_embeddings_PCA64.csv')\n",
    "# feed_embs[['feed_embed_' + str(col) for col in range(64)]] = mms.fit_transform(feed_embs[['feed_embed_' + str(col) for col in range(64)]])\n",
    "\n",
    "user_emb1 = np.load(FEATURE_PATH + '/user_emb_normal_50.npy')\n",
    "user_emb1 = torch.from_numpy(user_emb1).float().to(device)\n",
    "user_emb2 = np.load(FEATURE_PATH + '/user_emb_adjust_50.npy')\n",
    "user_emb2 = torch.from_numpy(user_emb2).float().to(device)\n",
    "\n",
    "hist_file = open(FEATURE_PATH + '/hist_data_action_begin_100.pkl', 'rb')\n",
    "hist_data = pickle.load(hist_file)\n",
    "hist_file.close()\n",
    "\n",
    "# feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "# feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "#     feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "# feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "# feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "\n",
    "# data = pd.read_csv(FEATURE_PATH + '/data_128.csv')\n",
    "\n",
    "if ONLINE_FLAG:\n",
    "#     data = pd.read_csv(USER_ACTION, iterator=True)\n",
    "    data = pd.read_csv(FEATURE_PATH + '/online_train_100.csv', iterator=True)\n",
    "    test = pd.read_csv(FEATURE_PATH + '/online_test_100.csv')\n",
    "else:\n",
    "    val = pd.read_csv(FEATURE_PATH + '/offline_val_100.csv')\n",
    "    data = pd.read_csv(FEATURE_PATH + '/offline_train_100.csv', iterator=True)\n",
    "\n",
    "# if ONLINE_FLAG:\n",
    "# #     data = pd.read_csv(USER_ACTION, iterator=True)\n",
    "# else:\n",
    "#     val = pd.read_csv(FEATURE_PATH + '/val_data.csv')\n",
    "#     data = pd.read_csv(FEATURE_PATH + '/train_data.csv', iterator=True)\n",
    "\n",
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=vocab_dict[feat] + 1, embedding_dim=embedding_dim)\n",
    "                      for feat in vocab_dict.keys()] + [DenseFeat(feat, 1) for feat in dense_features]\n",
    "\n",
    "fixlen_feature_columns += [SparseFeat('user_embedding_normal', 200000, embedding_dim=embedding_dim)]\n",
    "fixlen_feature_columns += [SparseFeat('user_embedding_adjust', 200000, embedding_dim=embedding_dim)]\n",
    "\n",
    "if ONLINE_FLAG:\n",
    "    # 加入test\n",
    "#     test = dt.fread(TEST_FILE)\n",
    "#     test = test.to_pandas()\n",
    "#     test = test.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "#                       on='feedid')\n",
    "#     test = test.merge(tag, how='left', on='feedid')\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_feedid', vocabulary_size=vocab_dict['feedid']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='test_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_authorid', vocabulary_size=vocab_dict['authorid']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='test_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_bgm_song_id', vocabulary_size=vocab_dict['bgm_song_id']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='test_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_bgm_singer_id', vocabulary_size=vocab_dict['bgm_singer_id']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='test_seq_len')]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    feature_names = get_feature_names(dnn_feature_columns)\n",
    "\n",
    "#     test[dense_features] = test[dense_features].fillna(0, )\n",
    "#     test[dense_features] = mms.fit_transform(test[dense_features])\n",
    "#     test['userid'] = userid_map.transform(test['userid'])\n",
    "\n",
    "    test_model_input = {name: test[name] for name in feature_names if 'hist' not in name and 'seq_len'not in name and name not in ['seq_len', 'feed_emb', 'feed_embedding', 'user_embedding_normal', 'user_embedding_adjust', 'hist_tagids', 'hist_keyids', 'tagids', 'keyids']}\n",
    "    test_model_input['user_embedding_normal'] = test_model_input['userid']\n",
    "    test_model_input['user_embedding_adjust'] = test_model_input['userid']\n",
    "#     test_model_input['hist_feedid'] = hist_data['hist_feedid'][test_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     test_model_input['hist_authorid'] = hist_data['hist_authorid'][test_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     test_model_input['hist_bgm_song_id'] = hist_data['hist_bgm_song_id'][test_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     test_model_input['hist_bgm_singer_id'] = hist_data['hist_bgm_singer_id'][test_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     test_model_input['test_seq_len'] = test['real_seq_len'].values\n",
    "#     test_model_input['test_seq_len'] = hist_data['test_seq_len'][test_model_input['userid']]\n",
    "else:\n",
    "#     val = val.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "#                   on='feedid')\n",
    "    \n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_feedid', vocabulary_size=vocab_dict['feedid']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='val_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_authorid', vocabulary_size=vocab_dict['authorid']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='val_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_bgm_song_id', vocabulary_size=vocab_dict['bgm_song_id']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='val_seq_len')]\n",
    "#     fixlen_feature_columns += [VarLenSparseFeat(SparseFeat('hist_bgm_singer_id', vocabulary_size=vocab_dict['bgm_singer_id']+1, embedding_dim=embedding_dim), max_hist_seq_len, length_name='val_seq_len')]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    feature_names = get_feature_names(dnn_feature_columns)\n",
    "    \n",
    "#     val[dense_features] = val[dense_features].fillna(0, )\n",
    "#     val[dense_features] = mms.fit_transform(val[dense_features])\n",
    "#     val['userid'] = userid_map.transform(val['userid'])\n",
    "    \n",
    "    val_model_input = {name: val[name] for name in feature_names if 'hist' not in name and 'seq_len' not in name and name not in ['feed_emb', 'hist_tagids', 'hist_keyids', 'feed_embedding', 'user_embedding_normal', 'user_embedding_adjust', 'tagids', 'keyids']}\n",
    "#     val_model_input['hist_feedid'] = hist_data['hist_feedid'][val_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     val_model_input['hist_authorid'] = hist_data['hist_authorid'][val_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     val_model_input['hist_bgm_song_id'] = hist_data['hist_bgm_song_id'][val_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     val_model_input['hist_bgm_singer_id'] = hist_data['hist_bgm_singer_id'][val_model_input['userid']][:, :max_hist_seq_len]\n",
    "#     val_model_input['val_seq_len'] = hist_data['val_seq_len'][val_model_input['userid']]\n",
    "#     val_model_input['val_seq_len'] = val['real_seq_len'].values\n",
    "\n",
    "    val_model_input['user_embedding_normal'] = val_model_input['userid']\n",
    "    val_model_input['user_embedding_adjust'] = val_model_input['userid']\n",
    "    userid_list = val['userid'].astype(str).tolist()\n",
    "    val_labels = [val[y].values for y in target]\n",
    "    \n",
    "\n",
    "print('use features: ', feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9c962a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee471bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse_features:  [SparseFeat(name='bgm_song_id', vocabulary_size=25160, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='bgm_song_id', group_name='default_group'), SparseFeat(name='bgm_singer_id', vocabulary_size=17501, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='bgm_singer_id', group_name='default_group'), SparseFeat(name='userid', vocabulary_size=200000, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='userid', group_name='default_group'), SparseFeat(name='feedid', vocabulary_size=112873, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='feedid', group_name='default_group'), SparseFeat(name='authorid', vocabulary_size=18790, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='authorid', group_name='default_group'), SparseFeat(name='device', vocabulary_size=4, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='device', group_name='default_group'), SparseFeat(name='user_embedding_normal', vocabulary_size=200000, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='user_embedding_normal', group_name='default_group'), SparseFeat(name='user_embedding_adjust', vocabulary_size=200000, embedding_dim=50, use_hash=False, dtype='int32', embedding_name='user_embedding_adjust', group_name='default_group')]\n",
      "varlen_features:  []\n",
      "chunk:  1\n",
      "cuda\n",
      "Train on 20000000 samples, validate on 0 samples, 19532 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1086it [01:30, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19532it [26:13, 12.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1573s - loss:  0.2592\n",
      "chunk:  2\n",
      "cuda\n",
      "Train on 20000000 samples, validate on 0 samples, 19532 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10292it [13:50, 13.02it/s]"
     ]
    }
   ],
   "source": [
    "# hist_features = ['feedid', 'authorid', 'bgm_singer_id', 'bgm_song_id']\n",
    "hist_features = []\n",
    "\n",
    "train_model = MMOE(dnn_feature_columns, history_feature_list=hist_features, num_tasks=7, num_experts=6, expert_dim=128, dnn_hidden_units=(512, 256, 64),\n",
    "                   task_dnn_units=(128, 128, 64),\n",
    "                   tasks=['binary', 'binary', 'binary', 'binary', 'binary', 'binary', 'binary'], device=device)\n",
    "train_model.compile(\"adagrad\", loss='binary_crossentropy')\n",
    "train_model.embedding_dict['user_embedding_normal'] = nn.Embedding.from_pretrained(user_emb1, freeze=False)\n",
    "train_model.embedding_dict['user_embedding_adjust'] = nn.Embedding.from_pretrained(user_emb2, freeze=False)\n",
    "\n",
    "loop = True\n",
    "cnt = 0\n",
    "while loop:\n",
    "    try:\n",
    "        cnt += 1\n",
    "        print('chunk: ', cnt)\n",
    "        train = data.get_chunk(2000*10000)\n",
    "        \n",
    "#         train = train.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "#                   on='feedid')\n",
    "#         train[dense_features] = train[dense_features].fillna(0, )\n",
    "#         train[dense_features] = mms.fit_transform(train[dense_features])\n",
    "#         train['userid'] = userid_map.transform(train['userid'])                \n",
    "\n",
    "        train_model_input = {name: train[name] for name in feature_names  if 'hist' not in name and 'seq_len'not in name and name not in ['feed_emb', 'hist_tagids', 'hist_keyids', 'feed_embedding', 'user_embedding_normal', 'user_embedding_adjust', 'tagids', 'keyids']}\n",
    "#         train_model_input['hist_feedid'] = hist_data['hist_feedid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_authorid'] = hist_data['hist_authorid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_bgm_song_id'] = hist_data['hist_bgm_song_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_bgm_singer_id'] = hist_data['hist_bgm_singer_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "        \n",
    "#         if ONLINE_FLAG:\n",
    "#             train_model_input['test_seq_len'] = hist_data['test_seq_len'][train_model_input['userid']]    \n",
    "# #             train_model_input['test_seq_len'] = train['real_seq_len'].values   \n",
    "\n",
    "#         else:\n",
    "#             train_model_input['val_seq_len'] = hist_data['val_seq_len'][train_model_input['userid']]    \n",
    "# #             train_model_input['val_seq_len'] = train['real_seq_len'].values \n",
    "\n",
    "        train_model_input['user_embedding_normal'] = train_model_input['userid']\n",
    "        train_model_input['user_embedding_adjust'] = train_model_input['userid']\n",
    "        train_labels = train[target].values\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            history = train_model.fit(train_model_input, train_labels,\n",
    "                              batch_size=batch_size, epochs=1, verbose=1, shuffle=True)\n",
    "        if not ONLINE_FLAG:\n",
    "            val_pred_ans = train_model.predict(val_model_input, batch_size=batch_size * 4)\n",
    "            # 模型predict()返回值格式为(?, 4)，与tf版mmoe不同。因此下方用到了transpose()进行变化。\n",
    "            evaluate_deepctr(val_labels, val_pred_ans.transpose(), userid_list, target)\n",
    "\n",
    "    except StopIteration:\n",
    "        loop=False\n",
    "        print('Finished all train')\n",
    "\n",
    "# 继续train一波 最近的数据\n",
    "# del data, train\n",
    "# data = pd.read_csv(FEATURE_PATH + '/online_train_100.csv', iterator=True)\n",
    "# train = data.get_chunk(4000*10000)\n",
    "# loop = True\n",
    "# cnt = 0\n",
    "# while loop:\n",
    "#     try:\n",
    "#         cnt += 1\n",
    "#         print('chunk: ', cnt)\n",
    "#         train = data.get_chunk(2000*10000)\n",
    "        \n",
    "# #         train = train.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "# #                   on='feedid')\n",
    "# #         train[dense_features] = train[dense_features].fillna(0, )\n",
    "# #         train[dense_features] = mms.fit_transform(train[dense_features])\n",
    "# #         train['userid'] = userid_map.transform(train['userid'])                \n",
    "\n",
    "#         train_model_input = {name: train[name] for name in feature_names  if 'hist' not in name and 'seq_len'not in name and name not in ['feed_emb', 'hist_tagids', 'hist_keyids', 'feed_embedding', 'user_embedding_normal', 'user_embedding_adjust', 'tagids', 'keyids']}\n",
    "# #         train_model_input['hist_feedid'] = hist_data['hist_feedid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "# #         train_model_input['hist_authorid'] = hist_data['hist_authorid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "# #         train_model_input['hist_bgm_song_id'] = hist_data['hist_bgm_song_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "# #         train_model_input['hist_bgm_singer_id'] = hist_data['hist_bgm_singer_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "        \n",
    "# #         if ONLINE_FLAG:\n",
    "# #             train_model_input['test_seq_len'] = hist_data['test_seq_len'][train_model_input['userid']]    \n",
    "# # #             train_model_input['test_seq_len'] = train['real_seq_len'].values   \n",
    "\n",
    "# #         else:\n",
    "# #             train_model_input['val_seq_len'] = hist_data['val_seq_len'][train_model_input['userid']]    \n",
    "# # #             train_model_input['val_seq_len'] = train['real_seq_len'].values \n",
    "\n",
    "#         train_model_input['user_embedding_normal'] = train_model_input['userid']\n",
    "#         train_model_input['user_embedding_adjust'] = train_model_input['userid']\n",
    "#         train_labels = train[target].values\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             history = train_model.fit(train_model_input, train_labels,\n",
    "#                               batch_size=batch_size, epochs=1, verbose=1, shuffle=True)\n",
    "#         if not ONLINE_FLAG:\n",
    "#             val_pred_ans = train_model.predict(val_model_input, batch_size=batch_size * 4)\n",
    "#             # 模型predict()返回值格式为(?, 4)，与tf版mmoe不同。因此下方用到了transpose()进行变化。\n",
    "#             evaluate_deepctr(val_labels, val_pred_ans.transpose(), userid_list, target)\n",
    "\n",
    "#     except StopIteration:\n",
    "#         loop=False\n",
    "#         print('Finished all train')\n",
    "\n",
    "\n",
    "if ONLINE_FLAG:\n",
    "    t1 = time()\n",
    "    pred_ans = train_model.predict(test_model_input, batch_size=batch_size * 20)\n",
    "    pred_ans = pred_ans.transpose()\n",
    "    t2 = time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0* 2000.0 / (len(test)*7.0) \n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "    # # 5.生成提交文件\n",
    "    for i, action in enumerate(target):\n",
    "        test[action] = pred_ans[i]\n",
    "    test['userid'] = userid_map.inverse_transform(test['userid'])\n",
    "    test[['userid', 'feedid'] + target].to_csv(SUBMIT_DIR + '/mmoe_cin_multi_all_1024_1.csv', index=None, float_format='%.6f')\n",
    "    print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89dacea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ORI:0.2309 0.671505\n",
    "\n",
    "#0.672874: cin 256 128 64\n",
    "# att_embedding_size=30\n",
    "#         att_head_num=10\n",
    "#         att_layer_num=3\n",
    "# 加上bi:674669 多train一些：0.676041\n",
    "# 多训练一次第12-13天：0.674816\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cea99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, train\n",
    "data = pd.read_csv(FEATURE_PATH + '/online_train_100.csv', iterator=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07d42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.get_chunk(1000*10000)\n",
    "train = data.get_chunk(4000*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5763574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk:  1\n",
      "cuda\n",
      "Train on 20000000 samples, validate on 0 samples, 19532 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19532it [26:14, 12.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1574s - loss:  0.2139\n",
      "chunk:  2\n",
      "cuda\n",
      "Train on 3175511 samples, validate on 0 samples, 3102 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3102it [04:03, 12.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "243s - loss:  0.2131\n",
      "chunk:  3\n",
      "Finished all train\n",
      "7个目标行为4252097条样本预测耗时（毫秒）：58330.128\n",
      "7个目标行为2000条样本平均预测耗时（毫秒）：3.919\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: [200001 200004 200005 ... 250242 250243 250244]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6c0eeb1f178e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_ans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muserid_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'userid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'feedid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSUBMIT_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/mmoe_cin_multi_all_1024_2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%.6f'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'to_csv ok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch_py3/lib/python3.6/site-packages/sklearn/preprocessing/_label.py\u001b[0m in \u001b[0;36minverse_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             raise ValueError(\n\u001b[0;32m--> 161\u001b[0;31m                     \"y contains previously unseen labels: %s\" % str(diff))\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: [200001 200004 200005 ... 250242 250243 250244]"
     ]
    }
   ],
   "source": [
    "# start: 3000\n",
    "# del train, data\n",
    "# data = pd.read_csv(FEATURE_PATH + '/offline_train_100.csv', iterator=True)\n",
    "loop = True\n",
    "cnt = 0\n",
    "while loop:\n",
    "    try:\n",
    "        cnt += 1\n",
    "        print('chunk: ', cnt)\n",
    "        train = data.get_chunk(2000*10000)\n",
    "        \n",
    "#         train = train.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "#                   on='feedid')\n",
    "#         train[dense_features] = train[dense_features].fillna(0, )\n",
    "#         train[dense_features] = mms.fit_transform(train[dense_features])\n",
    "#         train['userid'] = userid_map.transform(train['userid'])                \n",
    "\n",
    "        train_model_input = {name: train[name] for name in feature_names  if 'hist' not in name and 'seq_len'not in name and name not in ['feed_emb', 'hist_tagids', 'hist_keyids', 'feed_embedding', 'user_embedding_normal', 'user_embedding_adjust', 'tagids', 'keyids']}\n",
    "#         train_model_input['hist_feedid'] = hist_data['hist_feedid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_authorid'] = hist_data['hist_authorid'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_bgm_song_id'] = hist_data['hist_bgm_song_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "#         train_model_input['hist_bgm_singer_id'] = hist_data['hist_bgm_singer_id'][train_model_input['userid']][:, :max_hist_seq_len]\n",
    "        \n",
    "#         if ONLINE_FLAG:\n",
    "#             train_model_input['test_seq_len'] = hist_data['test_seq_len'][train_model_input['userid']]    \n",
    "# #             train_model_input['test_seq_len'] = train['real_seq_len'].values   \n",
    "\n",
    "#         else:\n",
    "#             train_model_input['val_seq_len'] = hist_data['val_seq_len'][train_model_input['userid']]    \n",
    "# #             train_model_input['val_seq_len'] = train['real_seq_len'].values \n",
    "\n",
    "        train_model_input['user_embedding_normal'] = train_model_input['userid']\n",
    "        train_model_input['user_embedding_adjust'] = train_model_input['userid']\n",
    "        train_labels = train[target].values\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            history = train_model.fit(train_model_input, train_labels,\n",
    "                              batch_size=batch_size, epochs=1, verbose=1, shuffle=True)\n",
    "        if not ONLINE_FLAG:\n",
    "            val_pred_ans = train_model.predict(val_model_input, batch_size=batch_size * 4)\n",
    "            # 模型predict()返回值格式为(?, 4)，与tf版mmoe不同。因此下方用到了transpose()进行变化。\n",
    "            evaluate_deepctr(val_labels, val_pred_ans.transpose(), userid_list, target)\n",
    "\n",
    "    except StopIteration:\n",
    "        loop=False\n",
    "        print('Finished all train')\n",
    "\n",
    "if ONLINE_FLAG:\n",
    "    t1 = time()\n",
    "    pred_ans = train_model.predict(test_model_input, batch_size=batch_size * 20)\n",
    "    pred_ans = pred_ans.transpose()\n",
    "    t2 = time()\n",
    "    print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "    ts = (t2 - t1) * 1000.0* 2000.0 / (len(test)*7.0) \n",
    "    print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n",
    "\n",
    "    # # 5.生成提交文件\n",
    "    for i, action in enumerate(target):\n",
    "        test[action] = pred_ans[i]\n",
    "    test['userid'] = userid_map.inverse_transform(test['userid'])\n",
    "    test[['userid', 'feedid'] + target].to_csv(SUBMIT_DIR + '/mmoe_cin_multi_all_1024_2.csv', index=None, float_format='%.6f')\n",
    "    print('to_csv ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d7f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['userid', 'feedid'] + target].to_csv(SUBMIT_DIR + '/mmoe_cin_multi_all_1024_2.csv', index=None, float_format='%.6f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94cd9f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa87d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
