{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d413c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Author:\n",
    "    zanshuxun, zanshuxun@aliyun.com\n",
    "    songwei, magic_24k@163.com\n",
    "\n",
    "Reference:\n",
    "    [1] [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from deepctr_torch.models.basemodel import BaseModel\n",
    "from deepctr_torch.inputs import combined_dnn_input\n",
    "from deepctr_torch.layers import DNN, PredictionLayer, CIN\n",
    "import pandas as pd\n",
    "\n",
    "class MMOELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    The Multi-gate Mixture-of-Experts layer in MMOE model\n",
    "      Input shape\n",
    "        - 2D tensor with shape: ``(batch_size,units)``.\n",
    "\n",
    "      Output shape\n",
    "        - A list with **num_tasks** elements, which is a 2D tensor with shape: ``(batch_size, output_dim)`` .\n",
    "\n",
    "      Arguments\n",
    "        - **input_dim** : Positive integer, dimensionality of input features.\n",
    "        - **num_tasks**: integer, the number of tasks, equal to the number of outputs.\n",
    "        - **num_experts**: integer, the number of experts.\n",
    "        - **output_dim**: integer, the dimension of each output of MMOELayer.\n",
    "\n",
    "    References\n",
    "      - [Jiaqi Ma, Zhe Zhao, Xinyang Yi, et al. Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts[C]](https://dl.acm.org/doi/10.1145/3219819.3220007)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_tasks, num_experts, output_dim):\n",
    "        super(MMOELayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        self.output_dim = output_dim\n",
    "        self.expert_network = nn.Linear(self.input_dim, self.num_experts * self.output_dim, bias=True)\n",
    "        self.gating_networks = nn.ModuleList(\n",
    "            [nn.Linear(self.input_dim, self.num_experts, bias=False) for _ in range(self.num_tasks)])\n",
    "        # initial model\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        expert_out = self.expert_network(inputs)\n",
    "        expert_out = expert_out.reshape([-1, self.output_dim, self.num_experts])\n",
    "        for i in range(self.num_tasks):\n",
    "            gate_out = self.gating_networks[i](inputs)\n",
    "            gate_out = gate_out.softmax(1).unsqueeze(-1)\n",
    "            output = torch.bmm(expert_out, gate_out).squeeze()\n",
    "            outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class MMOE(BaseModel):\n",
    "    \"\"\"Instantiates the Multi-gate Mixture-of-Experts architecture.\n",
    "\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param num_tasks: integer, number of tasks, equal to number of outputs, must be greater than 1.\n",
    "    :param tasks: list of str, indicating the loss of each tasks, ``\"binary\"`` for  binary logloss, ``\"regression\"`` for regression loss. e.g. ['binary', 'regression']\n",
    "    :param num_experts: integer, number of experts.\n",
    "    :param expert_dim: integer, the hidden units of each expert.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of shared-bottom DNN\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param init_std: float,to use as the initialize std of embedding vector\n",
    "    :param task_dnn_units: list,list of positive integer or empty list, the layer number and units in each layer of task-specific DNN\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param dnn_use_bn: bool. Whether use BatchNormalization before activation or not in DNN\n",
    "    :param device: str, ``\"cpu\"`` or ``\"cuda:0\"``\n",
    "\n",
    "    :return: A PyTorch model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dnn_feature_columns, num_tasks, tasks, num_experts=4, expert_dim=8, dnn_hidden_units=(128, 128),\n",
    "                 l2_reg_embedding=1e-5, l2_reg_dnn=0, init_std=0.0001, task_dnn_units=None, seed=1024, dnn_dropout=0,\n",
    "                 dnn_activation='relu', dnn_use_bn=False, device='cpu', gpus=[0, 1]):\n",
    "        \n",
    "        super(MMOE, self).__init__(linear_feature_columns=[], dnn_feature_columns=dnn_feature_columns,\n",
    "                                   l2_reg_embedding=l2_reg_embedding, seed=seed, device=device)\n",
    "        if num_tasks <= 1:\n",
    "            raise ValueError(\"num_tasks must be greater than 1\")\n",
    "        if len(tasks) != num_tasks:\n",
    "            raise ValueError(\"num_tasks must be equal to the length of tasks\")\n",
    "        for task in tasks:\n",
    "            if task not in ['binary', 'regression']:\n",
    "                raise ValueError(\"task must be binary or regression, {} is illegal\".format(task))\n",
    "\n",
    "        self.tasks = tasks\n",
    "        self.task_dnn_units = task_dnn_units\n",
    "        self.dnn = DNN(self.compute_input_dim(dnn_feature_columns), dnn_hidden_units,\n",
    "                       activation=dnn_activation, l2_reg=l2_reg_dnn, dropout_rate=dnn_dropout, use_bn=dnn_use_bn,\n",
    "                       init_std=init_std, device=device)\n",
    "        self.mmoe_layer = MMOELayer(dnn_hidden_units[-1], num_tasks, num_experts, expert_dim)\n",
    "        if task_dnn_units is not None:\n",
    "            # the last layer of task_dnn should be expert_dim\n",
    "            self.task_dnn = nn.ModuleList([DNN(expert_dim, task_dnn_units+(expert_dim,)) for _ in range(num_tasks)])\n",
    "        self.tower_network = nn.ModuleList([nn.Linear(expert_dim, 1, bias=False) for _ in range(num_tasks)])\n",
    "        self.out = nn.ModuleList([PredictionLayer(task) for task in self.tasks])\n",
    "        self.to(device)\n",
    "        \n",
    "        # 加入cin\n",
    "        cin_layer_size=(256, 128, 64,)\n",
    "        self.cin_layer_size=cin_layer_size\n",
    "        cin_split_half=True\n",
    "        cin_activation='relu'\n",
    "        l2_reg_cin=0\n",
    "        self.use_cin = len(self.cin_layer_size) > 0 and len(dnn_feature_columns) > 0\n",
    "        if self.use_cin:\n",
    "            field_num = len(self.embedding_dict)\n",
    "            if cin_split_half == True:\n",
    "                self.featuremap_num = sum(\n",
    "                    cin_layer_size[:-1]) // 2 + cin_layer_size[-1]\n",
    "            else:\n",
    "                self.featuremap_num = sum(cin_layer_size)\n",
    "            self.cin = CIN(field_num, cin_layer_size,\n",
    "                           cin_activation, cin_split_half, l2_reg_cin, seed, device=device)\n",
    "            self.cin_linear = nn.Linear(self.featuremap_num, 1, bias=False).to(device)\n",
    "            self.add_regularization_weight(filter(lambda x: 'weight' in x[0], self.cin.named_parameters()),\n",
    "                                           l2=l2_reg_cin)\n",
    "\n",
    "    def forward(self, X):\n",
    "        sparse_embedding_list, dense_value_list = self.input_from_feature_columns(X, self.dnn_feature_columns,\n",
    "                                                                           self.embedding_dict)\n",
    "        # 加入cin模块\n",
    "        if self.use_cin:\n",
    "            print(sparse_embedding_list)\n",
    "            cin_input = torch.cat(sparse_embedding_list, dim=1)\n",
    "            cin_output = self.cin(cin_input)\n",
    "            cin_logit = self.cin_linear(cin_output)\n",
    "        # 正常dnn\n",
    "        dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        mmoe_outs = self.mmoe_layer(dnn_output)\n",
    "        if self.task_dnn_units is not None:\n",
    "            mmoe_outs = [self.task_dnn[i](mmoe_out) for i, mmoe_out in enumerate(mmoe_outs)]\n",
    "\n",
    "        task_outputs = []\n",
    "        for i, mmoe_out in enumerate(mmoe_outs):\n",
    "            # cin的损失加入\n",
    "            logit = self.tower_network[i](mmoe_out) + cin_logit\n",
    "            output = self.out[i](logit)\n",
    "            task_outputs.append(output)\n",
    "\n",
    "        task_outputs = torch.cat(task_outputs, -1)\n",
    "        return task_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3433eb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please check the latest version manually on https://pypi.org/project/deepctr-torch/#history\n",
      "data.shape (73175511, 17)\n",
      "data.columns ['userid', 'feedid', 'date_', 'device', 'read_comment', 'comment', 'like', 'play', 'stay', 'click_avatar', 'forward', 'follow', 'favorite', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']\n",
      "unique date_:  [ 2  3  4  5  7  8  9 10 11  6 14  1 12 13]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import sys\n",
    "BASE_DIR = '.'\n",
    "sys.path.append(os.path.join(BASE_DIR, '../../config'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../model'))\n",
    "sys.path.append(os.path.join(BASE_DIR, '../utils'))\n",
    "from config import *\n",
    "from time import time\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datatable as dt\n",
    "from mmoe import MMOE\n",
    "from evaluation import evaluate_deepctr\n",
    "import pickle\n",
    "\n",
    "# 训练相关参数设置\n",
    "ONLINE_FLAG = False  # 是否准备线上提交\n",
    "\n",
    "# 指定GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "epochs = 3\n",
    "batch_size = 1024\n",
    "embedding_dim = 10\n",
    "target = ['read_comment', 'like', 'click_avatar', 'forward', 'comment', 'favorite', 'follow']\n",
    "# tagids = ['manual_tag_' + str(tagid) for tagid in range(11)] # 11\n",
    "# keyids = ['manual_key_' + str(keyid) for keyid in range(18)] # 18\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "dense_features = ['videoplayseconds', ]\n",
    "\n",
    "data = dt.fread(USER_ACTION)\n",
    "data = data.to_pandas()\n",
    "feed = dt.fread(FEED_INFO)\n",
    "feed = feed.to_pandas()\n",
    "# tag = dt.fread(FEATURE_PATH + '/feed_info_tags_keys_des.csv')\n",
    "# tag = tag.to_pandas()[tagids + keyids + ['feedid']]\n",
    "# feed_emb = np.load(FEATURE_PATH + '/feed_embeddings_PCA64.npy')\n",
    "# feed_emb = torch.from_numpy(feed_emb).float().to(device)\n",
    "# pkl = open(FEATURE_PATH + '/feedid_map.pkl', 'rb')\n",
    "# feedid_map = pickle.load(pkl)\n",
    "# pkl.close()\n",
    "\n",
    "feed[[\"bgm_song_id\", \"bgm_singer_id\"]] += 1  # 0 用于填未知\n",
    "feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]] = \\\n",
    "    feed[[\"bgm_song_id\", \"bgm_singer_id\", \"videoplayseconds\"]].fillna(0)\n",
    "feed['bgm_song_id'] = feed['bgm_song_id'].astype('int64')\n",
    "feed['bgm_singer_id'] = feed['bgm_singer_id'].astype('int64')\n",
    "data = data.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "                  on='feedid')\n",
    "# data['feedid'] = feedid_map.transform(data['feedid'])\n",
    "# data = data.merge(tag, how='left', on='feedid')\n",
    "# test = dt.fread(TEST_FILE)\n",
    "# test = test.to_pandas()\n",
    "# test = test.merge(feed[['feedid', 'authorid', 'videoplayseconds', 'bgm_song_id', 'bgm_singer_id']], how='left',\n",
    "#                   on='feedid')\n",
    "\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "# test[dense_features] = test[dense_features].fillna(0, )\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "# test[dense_features] = mms.fit_transform(test[dense_features])\n",
    "\n",
    "print('data.shape', data.shape)\n",
    "print('data.columns', data.columns.tolist())\n",
    "print('unique date_: ', data['date_'].unique())\n",
    "\n",
    "if ONLINE_FLAG:\n",
    "    train = data\n",
    "else:\n",
    "    train = data[data['date_'] < 14]\n",
    "val = data[data['date_'] == 14]  # 第14天样本作为验证集，当ONLINE_FLAG=False时使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d165ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Train on 67071556 samples, validate on 0 samples, 65500 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65500it [57:37, 18.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3457s - loss:  0.2591\n",
      "{'read_comment': 0.6281239123358137, 'like': 0.6222426049697215, 'click_avatar': 0.71415674846787, 'forward': 0.6826159944676095, 'comment': 0.5709973119489229, 'favorite': 0.7225344655460522, 'follow': 0.6890881204851775}\n",
      "Weighted uAUC:  0.651752\n",
      "cuda\n",
      "Train on 67071556 samples, validate on 0 samples, 65500 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65500it [57:40, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3460s - loss:  0.2405\n",
      "{'read_comment': 0.6344485807956676, 'like': 0.6234827659183279, 'click_avatar': 0.7157806271524819, 'forward': 0.6913918767484204, 'comment': 0.5799734092727782, 'favorite': 0.7289536672410949, 'follow': 0.6845573121278574}\n",
      "Weighted uAUC:  0.655745\n",
      "cuda\n",
      "Train on 67071556 samples, validate on 0 samples, 65500 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65500it [56:58, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3418s - loss:  0.2362\n",
      "{'read_comment': 0.6366060823656043, 'like': 0.6231750422031819, 'click_avatar': 0.7181202166078691, 'forward': 0.6947179428982619, 'comment': 0.5791335498635278, 'favorite': 0.7265931063021266, 'follow': 0.6837912814482804}\n",
      "Weighted uAUC:  0.656648\n"
     ]
    }
   ],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=embedding_dim)\n",
    "                          for feat in sparse_features] + [DenseFeat(feat, 1) for feat in dense_features]\n",
    "# fixlen_feature_columns += [SparseFeat(feat, vocabulary_size=350 + 1, embedding_dim=embedding_dim)\n",
    "#                           for feat in tagids]\n",
    "# fixlen_feature_columns += [SparseFeat(feat, vocabulary_size=23262 + 1, embedding_dim=embedding_dim)\n",
    "#                           for feat in keyids]\n",
    "# fixlen_feature_columns += [SparseFeat('feed_embedding', 112871+1, embedding_dim=64)]\n",
    "                    \n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(dnn_feature_columns)\n",
    "\n",
    "train_model_input = {name: train[name] for name in feature_names if name not in ['feed_embedding']}\n",
    "val_model_input = {name: val[name] for name in feature_names if name not in ['feed_embedding']}\n",
    "# train_model_input['feed_embedding'] = train_model_input['feedid']\n",
    "# val_model_input['feed_embedding'] = val_model_input['feedid']\n",
    "\n",
    "userid_list = val['userid'].astype(str).tolist()\n",
    "# test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "train_labels = train[target].values\n",
    "val_labels = [val[y].values for y in target]\n",
    "\n",
    "# 4.Define Model,train,predict and evaluate\n",
    "train_model = MMOE(dnn_feature_columns, num_tasks=7, num_experts=13, expert_dim=256, dnn_hidden_units=(128, 128, 128, 64),\n",
    "                   task_dnn_units=(128, 128, 64), \n",
    "#                    dnn_activation='dice',\n",
    "                   tasks=['binary', 'binary', 'binary', 'binary', 'binary', 'binary', 'binary'], device=device)\n",
    "# train_model.embedding_dict['feed_embedding'] = nn.Embedding.from_pretrained(feed_emb)\n",
    "\n",
    "train_model.compile(\"adagrad\", loss='binary_crossentropy')\n",
    "# print(train_model.summary())\n",
    "for epoch in range(epochs):\n",
    "    history = train_model.fit(train_model_input, train_labels,\n",
    "                              batch_size=batch_size, epochs=1, verbose=1)\n",
    "    if not ONLINE_FLAG:\n",
    "        val_pred_ans = train_model.predict(val_model_input, batch_size=batch_size * 4)\n",
    "        # 模型predict()返回值格式为(?, 4)，与tf版mmoe不同。因此下方用到了transpose()进行变化。\n",
    "        evaluate_deepctr(val_labels, val_pred_ans.transpose(), userid_list, target)\n",
    "\n",
    "# t1 = time()\n",
    "# pred_ans = train_model.predict(test_model_input, batch_size=batch_size * 20)\n",
    "# pred_ans = pred_ans.transpose()\n",
    "# t2 = time()\n",
    "# print('7个目标行为%d条样本预测耗时（毫秒）：%.3f' % (len(test), (t2 - t1) * 1000.0))\n",
    "# ts = (t2 - t1) * 1000.0* 2000.0 / (len(test)*7.0) \n",
    "# print('7个目标行为2000条样本平均预测耗时（毫秒）：%.3f' % ts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea8817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4573f0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93600b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
