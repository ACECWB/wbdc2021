{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error, accuracy_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Session(object):\n",
    "    def __init__(self, model, verbose=1):\n",
    "        self.model = model\n",
    "        self.verbose = verbose\n",
    "        self.weight = torch.tensor([1.6, 1.2, 0.8, 0.4, 0.4, 0.4, 0.4], dtype=torch.float32).to(model.device)\n",
    "    def compile(self, lr, optimizer, loss=None, metrics=None):\n",
    "        self.metrics_names = ['loss']\n",
    "        self.optim = self._get_optim(optimizer, lr=lr)\n",
    "        self.loss_func = self._get_loss(loss)\n",
    "        self.metrics = self._get_metrics(metrics)\n",
    "        \n",
    "    def multi_loss(self, y_pred, y_label):\n",
    "        loss = F.binary_cross_entropy(y_pred, y_label, reduction='none')\n",
    "        weight = torch.unsqueeze(self.weight, dim=0)\n",
    "        return torch.mean(loss * weight)\n",
    "\n",
    "    def train(self, train_loader):\n",
    "        self.model.train()\n",
    "        logs = {}\n",
    "        total_loss_epoch = 0.0\n",
    "        train_result = {}\n",
    "        sample_num = len(train_loader)\n",
    "        run_sample_num = [sample_num] * len(self.metrics)\n",
    "        \n",
    "        try:\n",
    "            with tqdm(enumerate(train_loader), disable=self.verbose != 1) as t:\n",
    "                for _, (train_x, train_y) in t:\n",
    "                    x = train_x.to(self.model.device).float()\n",
    "                    y = train_y.to(self.model.device).float()\n",
    "                    y_pred = self.model(x).squeeze()\n",
    "                    lls = self.loss_func(y_pred, y)\n",
    "                    reg_ls = self.model.get_regularization_loss()\n",
    "                    loss = lls + reg_ls + self.model.aux_loss\n",
    "                    total_loss_epoch += loss.item()\n",
    "                    self.optim.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optim.step()\n",
    "                    if self.verbose > 0:\n",
    "                        for i, data in enumerate(self.metrics.item()):\n",
    "                            name, metric_func = data[0], data[1]\n",
    "                            if name not in train_result:\n",
    "                                train_result[name] = []\n",
    "                            try:\n",
    "                                train_result[name].append(metric_func(y.cpu().data.numpy(), y_pred.cpu().data.numpy().astype('float32')))\n",
    "                            except ValueError:\n",
    "                                run_sample_num[i] = run_sample_num[i] - 1\n",
    "                                continue\n",
    "        except KeyboardInterrupt:\n",
    "            t.close()\n",
    "            raise\n",
    "        t.close()\n",
    "        logs['loss'] = total_loss_epoch / sample_num\n",
    "        for i, data in enumerate(train_result.item()):\n",
    "            name, result = data[0], data[1]\n",
    "            losg[name] = np.sum(result) / (sample_num - run_sample_num[i])\n",
    "        return logs\n",
    "    \n",
    "    def evaluate(self, val_x, val_y, table, batch_size=1024):\n",
    "        eval_result = {}\n",
    "        val_tensor_data = self.model.generate_loader(val_x, None, table, None)\n",
    "        val_loader = DataLoader(val_tensor_data, shuffle=False, batch_size=batch_size)\n",
    "        pred_ans = self.predict(val_loader)\n",
    "        for name, metric_func in self.metrics.items():\n",
    "            eval_result[name] = metric_func(pred_ans, val_y)\n",
    "        return eval_result\n",
    "        \n",
    "    def predict(self, val_loader):\n",
    "        self.model.eval()\n",
    "        pred_ans = []\n",
    "        with torch.no_grad():\n",
    "            for t in val_loader:\n",
    "                x = t.to(self.model.device).float()\n",
    "                y_pred = self.model(x).cpu().data.numpy()\n",
    "                pred_ans.append(y_pred)\n",
    "        return np.concatenate(y_pred).astype('float64')\n",
    "    \n",
    "    def _get_metrics(self, metrics, set_eps=False):\n",
    "        metrics_ = {}\n",
    "        if metrics:\n",
    "            for metric in metrics:\n",
    "                if metric == \"binary_crossentropy\" or metric == \"logloss\":\n",
    "                    if set_eps:\n",
    "                        metrics_[metric] = self._log_loss\n",
    "                    else:\n",
    "                        metrics_[metric] = log_loss\n",
    "                if metric == \"auc\":\n",
    "                    metrics_[metric] = roc_auc_score\n",
    "                if metric == \"mse\":\n",
    "                    metrics_[metric] = mean_squared_error\n",
    "                if metric == \"accuracy\" or metric == \"acc\":\n",
    "                    metrics_[metric] = lambda y_true, y_pred: accuracy_score(\n",
    "                        y_true, np.where(y_pred > 0.5, 1, 0))\n",
    "                self.metrics_names.append(metric)\n",
    "        return metrics_\n",
    "        \n",
    "    \n",
    "    def _get_optim(self, optimizer, **kwargs):\n",
    "        if isinstance(optimizer, str):\n",
    "            lr = kwargs['lr']\n",
    "            if optimizer == \"sgd\":\n",
    "                optim = torch.optim.SGD(self.model.parameters(), lr=lr)\n",
    "            elif optimizer == \"adam\":\n",
    "                optim = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "            elif optimizer == \"adagrad\":\n",
    "                optim = torch.optim.Adagrad(self.model.parameters(), lr=lr)\n",
    "            elif optimizer == \"rmsprop\":\n",
    "                optim = torch.optim.RMSprop(self.model.parameters(), lr=lr)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "        else:\n",
    "            optim = optimizer\n",
    "\n",
    "        return optim\n",
    "    def _get_loss(self, loss):\n",
    "        if isinstance(loss, str):\n",
    "            if loss == \"binary_crossentropy\":\n",
    "                loss_func = F.binary_cross_entropy\n",
    "            elif loss == \"multi_binary_crossentroy\":\n",
    "                loss_func = self.multi_loss\n",
    "            elif loss == \"mse\":\n",
    "                loss_func = F.mse_loss\n",
    "            elif loss == \"mae\":\n",
    "                loss_func = F.l1_loss\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "        else:\n",
    "            loss_func = loss\n",
    "        return loss_func\n",
    "    def _log_loss(self, y_true, y_pred, eps=1e-7, normalize=True, sample_weight=None, labels=None):\n",
    "        # change eps to improve calculation accuracy\n",
    "        return log_loss(y_true, y_pred, eps, normalize, sample_weight, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_wbdc",
   "language": "python",
   "name": "conda_wbdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
