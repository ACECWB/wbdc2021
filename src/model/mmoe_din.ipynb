{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/opt/spark-2.4.5-bin-hadoop2.7/python', '/home/tione/notebook/wbdc2021-preliminary-48c2b28c233f4934b362696daef770e4/src/model', '/home/tione/notebook/envs/wbdc/lib/python36.zip', '/home/tione/notebook/envs/wbdc/lib/python3.6', '/home/tione/notebook/envs/wbdc/lib/python3.6/lib-dynload', '/home/tione/notebook/envs/wbdc/lib/python3.6/site-packages', '/home/tione/notebook/envs/wbdc/lib/python3.6/site-packages/IPython/extensions', '/home/tione/.ipython', '../utils', '..']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "sys.path.append('..')\n",
    "from model.sequence import AttentionSequencePoolingLayer\n",
    "from inputs import *\n",
    "from model.core import PredictionLayer, DNN\n",
    "from dataset import CustomerTensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateLayer(nn.Linear):\n",
    "    def __init__(self, input_dim, num_expert, device='cpu'):\n",
    "        super(GateLayer, self).__init__(input_dim, num_expert)\n",
    "        self.input_dim = input_dim\n",
    "        self.num_expert = num_expert\n",
    "        self.act = nn.Softmax(dim=1)\n",
    "        self.to(device)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.act(self.forward(x))\n",
    "\n",
    "class DeepInterestNetworkMMoe(nn.Module):\n",
    "    def __init__(self, dnn_feature_columns, history_feature_list, seq_length_list,\n",
    "                 att_hidden_size=(64, 32), att_activation='Dice', att_weight_normalization=False,\n",
    "                 dnn_share_bottom_units=(256,), dnn_hidden_expert_units=(128, ), dnn_hidden_units=(64, ),\n",
    "                 dnn_activation='relu', dnn_dropout=0.1, act_dropout=0.0, l2_reg_dnn=1e-5, l2_reg_embedding=1e-5,\n",
    "                 dnn_use_bn=False, task='binary', num_task=7, num_expert=5, device='cpu'):\n",
    "        super(DeepInterestNetworkMMoe, self).__init__()\n",
    "        self.num_task = num_task\n",
    "        self.num_expert = num_expert\n",
    "        self.dnn_feature_columns = dnn_feature_columns\n",
    "        self.history_feature_list = history_feature_list\n",
    "        self.seq_length_list = seq_length_list\n",
    "        self.device = device\n",
    "        \n",
    "        self.aux_loss = torch.zeros((1,), device=device)\n",
    "        \n",
    "        self.sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        self.varlen_sparse_feature_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        self.dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), dnn_feature_columns)) if dnn_feature_columns else []\n",
    "        self.history_feature_columns = []\n",
    "        self.sparse_varlen_feature_columns = []\n",
    "        self.history_fc_names = list(map(lambda x: \"hist_\" + x, history_feature_list))\n",
    "        for fc in varlen_sparse_feature_columns:\n",
    "            feat_name = fc.name\n",
    "            if feat_name in self.history_fc_names:\n",
    "                self.history_feature_columns.append(fc)\n",
    "            else:\n",
    "                self.sparse_varlen_feature_columns.append(fc)\n",
    "                \n",
    "        self.embedding_dict = self._create_embedding_matrix(dnn_feature_columns)\n",
    "        self.feature_index = self._build_input_features(dnn_feature_columns)\n",
    "        att_emb_dim = self._compute_interest_dim()\n",
    "        dnn_in_dim = self._compute_input_dim(dnn_feature_columns)\n",
    "        \n",
    "        self.attention = AttentionSequencePoolingLayer(att_hidden_size=att_hidden_size,\n",
    "                                                       embedding_dim=att_emb_dim,\n",
    "                                                       att_activation=att_activation,\n",
    "                                                       return_score=False,\n",
    "                                                       dropout_rate=act_dropout,\n",
    "                                                       supports_masking=False,\n",
    "                                                       weight_normalization=att_weight_normalization)\n",
    "        self.dnn_share_bottom = DNN(inputs_dim=dnn_in_dim,\n",
    "                                    hidden_units=dnn_share_bottom_units,\n",
    "                                    activation=dnn_activation,\n",
    "                                    dropout_rate=dnn_dropout,\n",
    "                                    use_bn=dnn_use_bn)\n",
    "        self.dnn_expert = nn.ModuleList([\n",
    "            DNN(inputs_dim=dnn_share_bottom_units[-1],\n",
    "                hidden_units=dnn_hidden_expert_units,\n",
    "                activation=dnn_activation,\n",
    "                dropout_rate=dnn_dropout,\n",
    "                use_bn=dnn_use_bn)\n",
    "            for _ in range(num_expert)\n",
    "        ])\n",
    "        \n",
    "        self.gate_list = nn.ModuleList([\n",
    "            GateLayer(dnn_hidden_units[-1], num_expert) for _ in range(num_task)\n",
    "        ])\n",
    "        \n",
    "        self.dnn_task = nn.ModuleList([\n",
    "            DNN(inputs_dim=dnn_hidden_expert_units[-1],\n",
    "                hidden_units=dnn_hidden_units,\n",
    "                activation=dnn_activation,\n",
    "                dropout_rate=dnn_dropout,\n",
    "                use_bn=dnn_use_bn)\n",
    "            for _ in range(num_task)\n",
    "        ])\n",
    "        \n",
    "        self.dnn_task_output = nn.ModuleList([\n",
    "            nn.Linear(dnn_hidden_units[-1], 1)\n",
    "            for _ in range(num_task)\n",
    "        ])\n",
    "        \n",
    "        self.dnn_task_predict = nn.ModuleList([\n",
    "            PredictionLayer(task)\n",
    "            for _ in range(num_task)\n",
    "        ])\n",
    "        \n",
    "        self.regularization_weight = []\n",
    "        self.add_regularization_weight(self.embedding_dict.parameters(), l2=l2_reg_embedding)\n",
    "        self.add_regularization_weight(self.dnn_task.parameters(), l2=l2_reg_dnn)\n",
    "        self.add_regularization_weight(self.dnn_expert.parameters(), l2=l2_reg_dnn)\n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        _, dense_value_list = self._input_from_feature_columns(X, self.dnn_feature_columns, self.embedding_dict)\n",
    "        query_emb_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.sparse_feature_columns,\n",
    "                                          return_feat_list=self.history_feature_list, to_list=True)\n",
    "        keys_emb_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.history_feature_columns,\n",
    "                                         return_feat_list=self.history_fc_names, to_list=True)\n",
    "        dnn_input_emb_list = embedding_lookup(X, self.embedding_dict, self.feature_index, self.sparse_feat_columns, to_list=True)\n",
    "        sequence_embed_dict = varlen_embedding_lookup(X, self.embedding_dict, self.feature_index, self.sparse_varlen_feature_columns)\n",
    "        sequence_emb_list = get_varlen_pooling_list(sequence_embed_dict, X, self.feature_index, self.sparse_varlen_feature_columns, self.device)\n",
    "        \n",
    "        dnn_input_emb_list += sequence_emb_list\n",
    "        deep_input_emb = torch.cat(dnn_input_emb_list, dim=-1)\n",
    "        \n",
    "        query_emb = torch.cat(query_emb_list, dim=-1)\n",
    "        key_emb = torch.cat(keys_emb_list, dim=-1)\n",
    "        key_length_feature_name = [feat.length_name for feat in self.varlen_sparse_feature_columns if feat.length_name is not None]\n",
    "        key_length = torch.squeeze(maxlen_lookup(X, self.feature_index, key_length_feature_name), 1)\n",
    "        hist = self.attention(query_emb, key_emb, key_length)\n",
    "        deep_input_emb = torch.cat((deep_input_emb, hist), dim=-1)\n",
    "        deep_input_emb = deep_input_emb.view((deep_input_emb.shape[0], -1))\n",
    "        dnn_input = combined_dnn_input([deep_input_emb], dense_value_list)\n",
    "        share_bottom_output = self.dnn_share_bottom(dnn_input)\n",
    "        gate_outputs = [self.gate_list[k](share_bottom_output) for k in range(num_task)]\n",
    "        dnn_expert_outputs = [self.dnn_expert[k](share_bottom_output) for k in range(num_task)]\n",
    "        tower_input = [torch.mean(torch.unsqueeze(weight, dim=1) * torch.stack(dnn_expert_outputs, dim=-1),dim=-1) for weight in gate_outputs]\n",
    "        outputs = []\n",
    "        for i in range(num_task):\n",
    "            inputs = tower_input[i]\n",
    "            out = self.dnn_task[i](inputs)\n",
    "            out = self.dnn_task_output[i](out)\n",
    "            out = self.dnn_task_predict[i](out)\n",
    "            outputs += [out]\n",
    "        return torch.cat(outputs, dim=1)\n",
    "        \n",
    "    \n",
    "    def _input_from_feature_columns(self, X, feature_columns, embedding_dict, support_dense=True):\n",
    "        sparse_feat_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_feat_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        varlen_feat_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        sparse_emb_list = [embedding_dict[feat.embedding_name](\n",
    "            X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long()) for feat in sparse_feat_columns]\n",
    "        dense_value_list = [X[:, self.feature_index[feat.name][0]:self.feature_index[feat.name][1]].long() for feat in dense_feat_columns]\n",
    "        varlen_emb_list = get_varlen_pooling_list(self.embedding_dict, X, self.feature_index, varlen_feat_columns, self.device)\n",
    "        return sparse_emb_list + varlen_emb_list, dense_value_list\n",
    "        \n",
    "    def _create_embedding_matrix(self, feature_columns, init_std=0.0001, linear=False, sparse=False, device='cpu'):\n",
    "        sparse_feat_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        varlen_sparse_columns = list(filter(lambda x: isinstance(x, VarLenSparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        embedding_dict = nn.ModuleDict({\n",
    "            feat.embedding_name: nn.Embedding(feat.vocabulary_size, feat.embedding_dim if not linear else 1, sparse=sparse)\n",
    "            for feat in sparse_feat_columns+varlen_sparse_columns\n",
    "        })\n",
    "        for tensor in embedding_dict.values():\n",
    "            nn.init.xavier_normal_(tensor.weight)\n",
    "        return embedding_dict.to(device)\n",
    "    \n",
    "    def _build_input_features(self, feature_columns):\n",
    "        features = OrderedDict()\n",
    "        start = 0\n",
    "        for feat in feature_columns:\n",
    "            feat_name = feat.name\n",
    "            if feat_name in features:\n",
    "                continue\n",
    "            if isinstance(feat, SparseFeat):\n",
    "                features[feat_name] = (start, start+1)\n",
    "                start += 1\n",
    "            elif isinstance(feat, DenseFeat):\n",
    "                features[feat_name] = (start, start+feat.dimension)\n",
    "                start += feat.dimension\n",
    "            elif isinstance(feat, VarLenSparseFeat):\n",
    "                features[feat_name] = (start, start+feat.maxlen)\n",
    "                start += feat.maxlen\n",
    "                if feat.length_name not in features and feat.length_name is not None:\n",
    "                    features[feat.length_name] = (start, start+1)\n",
    "                    start += 1\n",
    "            else:\n",
    "                raise TypeError(\"Invalid feature column type,got\", type(feat))\n",
    "        return features\n",
    "    def _compute_interest_dim(self):\n",
    "        interest_dim = 0\n",
    "        for feat in self.sparse_feat_columns:\n",
    "            if feat.name in self.history_feature_list:\n",
    "                interest_dim += feat.embedding_dim\n",
    "        return interest_dim\n",
    "    \n",
    "    def _compute_input_dim(self, feature_columns, include_sparse=True, include_dense=True, feature_group=False):\n",
    "        sparse_feat_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_feat_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns)) if len(feature_columns) else []\n",
    "        dense_input_dim = sum(map(lambda x: x.dimension, dense_feat_columns))\n",
    "        if feature_group:\n",
    "            sparse_input_dim = len(sparse_feat_columns)\n",
    "        else:\n",
    "            sparse_input_dim = sum(map(lambda x: x.embedding_dim, sparse_feat_columns))\n",
    "        input_dim = 0\n",
    "        if include_dense:\n",
    "            input_dim += dense_input_dim\n",
    "        if include_sparse:\n",
    "            input_dim += sparse_input_dim\n",
    "        return input_dim\n",
    "    def add_regularization_weight(self, weight_list, l1=0.0, l2=0.0):\n",
    "        if isinstance(weight_list, torch.nn.parameter.Parameter):\n",
    "            weight_list = [weight_list]\n",
    "        else:\n",
    "            weight_list = list(weight_list)\n",
    "        self.regularization_weight.append((weight_list, l1, l2))\n",
    "        \n",
    "    def get_regularization_loss(self):\n",
    "        total_reg_loss = torch.zeros((1,), device=self.device)\n",
    "        for weight, l1, l2 in self.regularization_weight:\n",
    "            for w in weight:\n",
    "                if isinstance(weight, tuple):\n",
    "                    parameter = w[1]\n",
    "                else:\n",
    "                    paramter = w\n",
    "                if l1 > 0:\n",
    "                    total_reg_loss += torch.sum(torch.abs(parameter) * l1)\n",
    "                if l2 > 0:\n",
    "                    try:\n",
    "                        total_reg_loss += torch.sum(l2 * torch.square(parameter))\n",
    "                    except AttributeError:\n",
    "                        total_reg_loss += torch.sum(l2 * parameter * parameter)\n",
    "        return total_reg_loss\n",
    "    def generate_loader(self, x, y, table=None, return_table=None):\n",
    "        x_, x_table = [], []\n",
    "        if isinstance(x, dict):\n",
    "            for feat in self.feature_index:\n",
    "                if feat in self.history_fc_names or feat in self.seq_length_list:\n",
    "                    x_table.append(x[feat])\n",
    "                else:\n",
    "                    x_.append(x[feat])\n",
    "        for i in range(len(x_)):\n",
    "            if len(x_[i].shape) == 1:\n",
    "                x_[i] = np.expand_dims(x_[i], axis=1)\n",
    "        for i in range(len(x_table)):\n",
    "            if len(x_table[i]) == 1:\n",
    "                x_table[i] = np.expand_dims(x_table[i], axis=1)\n",
    "        if table is not None:\n",
    "            x_table = table\n",
    "        if y is not None:\n",
    "            y = torch.from_numpy(y)\n",
    "        train_tensor_data = CustomerTensorDataset(torch.from_numpy(np.concatenate(x_, axis=-1)), y, x_table)\n",
    "        if return_table:\n",
    "            return train_tensor_data, x_table\n",
    "        else:\n",
    "            return train_tensor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_py3",
   "language": "python",
   "name": "conda_pytorch_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
